{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc245902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('MitreData.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559950da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['combined'] = df.iloc[:, df.columns != 'id']\n",
    "df['combined'] = df.iloc[:,df.columns != 'id'].apply(lambda x: \",\".join(x.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb71f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c361528",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['combined']\n",
    "y_train = df['id']\n",
    "train_X =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def pre_process(text):\n",
    "    \n",
    "#     # lowercase\n",
    "#     text=text.lower()\n",
    "    \n",
    "#     #remove tags\n",
    "#     text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "#     # remove special characters and digits\n",
    "#     text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# X_train = X_train.apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a772817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8045c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('nan')\n",
    "\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "for sent in X_train:\n",
    "    for ele in sent:\n",
    "        if ele in punc:\n",
    "            sent = contractions.fix(sent)\n",
    "            sent = sent.replace(ele, \" \").strip().lower()\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    for fs in range(len(filtered_sentence)):\n",
    "        filtered_sentence[fs] = porter.stem(filtered_sentence[fs])\n",
    "    #print(filtered_sentence)\n",
    "    train_X.append(\" \".join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36126aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(train_X)\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ea238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#naive bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c41f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"Suspicious Process Activity - Potential Mimikatz or Hash Passing Token Creation - Powershell Privileged Service Call Analytic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "test_processed = []\n",
    "for sent in test:\n",
    "  for ele in sent:\n",
    "      if ele in punc:\n",
    "          sent = contractions.fix(sent)\n",
    "          sent = sent.replace(ele, \" \").strip().lower()\n",
    "  word_tokens = word_tokenize(sent)\n",
    "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "  for fs in range(len(filtered_sentence)):\n",
    "    filtered_sentence[fs] = porter.stem(filtered_sentence[fs])\n",
    "  #print(filtered_sentence)\n",
    "  test_processed.append(\" \".join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tf_idf.transform(test_processed)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=naive_bayes_classifier.predict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = naive_bayes_classifier.score(X_train_tf, y_train)\n",
    "print(score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9be46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
